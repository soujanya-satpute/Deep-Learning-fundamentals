{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Weights and Gradient",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "`get_slope()` function that takes `input_data`, `target`, and `weights` as arguments. There is also a `get_mse()` function that takes the same arguments. The `input_data`, `target`, and `weights` have been pre-loaded.\n",
        "\n",
        "This network does not have any hidden layers, and it goes directly from the input (with 3 nodes) to an output node. Note that weights is a single array.\n",
        "\n",
        "We have also pre-loaded matplotlib.pyplot, and the error history will be plotted after you have done your gradient descent steps."
      ],
      "metadata": {
        "id": "RkD6rCQh7g9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_updates = 20\n",
        "mse_hist = []\n",
        "\n",
        "# Iterate over the number of updates\n",
        "for i in range(n_updates):\n",
        "    # Calculate the slope: slope\n",
        "    slope = get_slope(input_data, target, weights)\n",
        "    \n",
        "    # Update the weights: weights\n",
        "    weights = weights - (0.01*slope)\n",
        "    \n",
        "    # Calculate mse with new weights: mse\n",
        "    mse = get_mse(input_data, target, weights)\n",
        "    \n",
        "    # Append the mse to mse_hist\n",
        "    mse_hist.append(mse)\n",
        "\n",
        "# Plot the mse history\n",
        "plt.plot(mse_hist)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bfGPTO-17zeH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}